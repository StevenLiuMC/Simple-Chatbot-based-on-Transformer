# -*- coding: utf-8 -*-
"""Transformer based Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aKgtkJaSs9QMwpbNM21iHjBQ0ANKDNvy
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import json
import re
from tqdm import tqdm
import math
import random

# Set device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# ============== Tokenizer ==============

class SimpleTokenizer:
  """
  A simple character or word-level tokenizer for the chatbot.
  This tokenizer builds a vocabulary from the training data and converts between text and tokens.
  For this simple tokenizer we only have two granularities - word or character. Subword level is not supported.
  """
  def __init__(self, level = "word"):
    """
    Args:
      level: "word" or "char" - determines tokenization granularity 决定分词粒度
    """
    self.level = level
    self.word2idx = {'<PAD>':0, '<UNK>':1, '<SOS>':2, '<EOS>':3} # This is a common practice in NLP tasks.
    self.idx2word = {0:'<PAD>', 1:'<UNK>', 2:'<SOS>', 3:'<EOS>'}
    self.vocab_size = 4 # Indicates the current vocabulary size, initially there are only 4 special tokens

  def build_vocab(self, texts, max_vocab_size = 10000): # The vocabulary contains at most 10,000 different words.
    """
    Build vocabulary from a list of texts.
    Args:
      texts: List of text strings to build vocabulary from.
      max_vocab_size: Maximum vocabulary size to keep.
    """
    word_freq = {}

    for text in texts:
      tokens = self._tokenize(text)
      for token in tokens:
        word_freq[token] = word_freq.get(token, 0) + 1 # dict.get(key, default=None)

    # Sort by frequency and keep top max_vocab_size
    sorted_words = sorted(word_freq.items(), key = lambda x: x[1], reverse = True)
    sorted_words = sorted_words[:max_vocab_size - 4] # Reserve 4 spots for special tokens

    # Add to vocabulary， insert them into the word2idx dictionary in descending order of word frequency.
    for word,_ in sorted_words:
      if word not in self.word2idx:
        self.word2idx[word] = self.vocab_size
        self.idx2word[self.vocab_size] = word
        self.vocab_size += 1
    print(f"Vocabulary size: {self.vocab_size}")

    def _tokenize(self, text):
      """
      Tokenize text based on level (in this case we use word-level).
      """
      text = text.lower().strip()
      if self.level == "word":
        # Simple word tokenization - you can improve this with better regex
        # Match one or more letters/numbers/underscores (words) or matches a non-alphanumeric and non-whitespace character (punctuation)
        tokens = re.findall(r'\w+|[^\w\s]',text) # \w+ 匹配一个或多个字母/数字/下划线（单词），| 表示"或"，[^\w\s] 匹配非字母数字且非空白的字符（标点符号）
      else: # Char level
        tokens = list(text)
      return tokens

    def encode(self, text, max_length = None):
      """
      Convert text to token indices.
      Args:
        text: Input text string to encode.
        max_length: Maximum sequence length (pad or truncate to this length).
      Returns:
        List of token indices.
      """
      tokens = self._tokenize(text)
      indices = [self.word2idx.get(token, self.word2idx['<UNK>']) for token in tokens]

      if max_length:
        # Truncate and remain one spot for EOS
        if len(indices) >= max_length - 1:
          indices = indices[:max_length-1]

        # If sequence is not long enough, add EOS first to tell the model that the useful information has ended and then add PAD to make the indices dimension unified.
        indices.append(self.word2idx['<EOS>'])

        if len(indices) < max_length:
          indices += [self.word2idx['<PAD>']] * (max_length - len(indices))
        else:
        # If there's no limitation on length, simply add EOS
          indices.append(self.word2idx['<EOS>'])

      return indices

    def decode(self, indices):
      """
      Convert token indices back to text.
      Args:
        indices: List or tensor of token indices.
      Returns:
        Decoded text string.
      """
      if torch.is_tensor(indices):
        indices = indices.cpu().numpy()

      tokens = []
      for idx in indices:
        if idx == self.word2idx['<EOS>']:
          break
        if idx != self.word2idx['<PAD>'] and idx != self.word2idx['<SOS>']:
          tokens.append(self.idx2word.get(idx, '<UNK>'))

      if self.level == "word":
        return ' '.join(tokens)
      else:
        return ''.join(tokens)

